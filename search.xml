<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>AI-for-beginners-1_lab</title>
      <link href="/2024/08/21/AI-for-beginners-1-lab/"/>
      <url>/2024/08/21/AI-for-beginners-1-lab/</url>
      
        <content type="html"><![CDATA[<h1>Task</h1><p>使用上一篇讲述到的代码做一个MNIST的手写数字分类的模型：能够做多类别分类来识别任何数字。<br>给train data和test data计算准确率，输出confusion矩阵</p><h1>Hint</h1><p>1、对每一个数字：创建一个二元分类器： this digit <strong>vs</strong> all other digit<br>2、给每一个数字创建不同的二元分类感受器<br>3、定义一个函数来分类输入的数字</p><blockquote><p>如果外面将10个感受器的权重合到一个矩阵上，我们就能将10个感受器的结果通过一次矩阵乘法完成。最可能的数字将通过argmax操作输出出来</p></blockquote><h1>Think</h1><p>前文Perceptron一节中我们构建了一个二元的分类器，即可以分辨两个数字的的神经网络，那么我们分辨10个数字的神经网络便在此基础上来改造<br>最直接的想法就是提示中给我们的思路：仍然使用一个二元分类器，用于分类<strong>这个数字</strong>和<strong>其他数字</strong><br>于是乎答案呼之欲出了，我们仍然用一个特定的数字i来作为<strong>这个数字</strong>的数据，对于<strong>其他数字</strong>的数据，我们则将0~9中除了i以外的所有数字的数据混合在一起，就得到了<strong>其他数字</strong>的数据集，随后我们用同样的方法训练一个二元分类器，看上去我们的任务就将圆满完成了！！</p><h1>Let’s do it</h1><h2 id="neccessary-lib">neccessary lib</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure><h2 id="copy-functions">copy functions</h2><p>我们直接把Perceptron中的训练函数摘抄过来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">positive_examples, negative_examples, num_iterations = <span class="number">100</span></span>):</span><br><span class="line">    num_dims = positive_examples.shape[<span class="number">1</span>]</span><br><span class="line">    weights = np.zeros((num_dims,<span class="number">1</span>)) <span class="comment"># initialize weights</span></span><br><span class="line">    </span><br><span class="line">    pos_count = positive_examples.shape[<span class="number">0</span>]</span><br><span class="line">    neg_count = negative_examples.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    report_frequency = <span class="number">10</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iterations):</span><br><span class="line">        pos = random.choice(positive_examples)</span><br><span class="line">        neg = random.choice(negative_examples)</span><br><span class="line"></span><br><span class="line">        z = np.dot(pos, weights)   </span><br><span class="line">        <span class="keyword">if</span> z &lt; <span class="number">0</span>:</span><br><span class="line">            weights = weights + pos.reshape(weights.shape)</span><br><span class="line"></span><br><span class="line">        z  = np.dot(neg, weights)</span><br><span class="line">        <span class="keyword">if</span> z &gt;= <span class="number">0</span>:</span><br><span class="line">            weights = weights - neg.reshape(weights.shape)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> i % report_frequency == <span class="number">0</span>:             </span><br><span class="line">            pos_out = np.dot(positive_examples, weights)</span><br><span class="line">            neg_out = np.dot(negative_examples, weights)        </span><br><span class="line">            pos_correct = (pos_out &gt;= <span class="number">0</span>).<span class="built_in">sum</span>() / <span class="built_in">float</span>(pos_count)</span><br><span class="line">            neg_correct = (neg_out &lt; <span class="number">0</span>).<span class="built_in">sum</span>() / <span class="built_in">float</span>(neg_count)</span><br><span class="line">            <span class="comment"># print(&quot;Iteration=&#123;&#125;, pos correct=&#123;&#125;, neg correct=&#123;&#125;&quot;.format(i,pos_correct,neg_correct))</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure><p>以及判断分类器正确率的函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">weights, test_x, test_labels</span>):</span><br><span class="line">    ones = np.c_[test_x,np.ones(<span class="built_in">len</span>(test_x))]</span><br><span class="line">    res = np.dot(ones , weights)</span><br><span class="line">    <span class="keyword">return</span> (res.reshape(test_labels.shape)*test_labels&gt;=<span class="number">0</span>).<span class="built_in">sum</span>()/<span class="built_in">float</span>(<span class="built_in">len</span>(test_labels))</span><br></pre></td></tr></table></figure><h2 id="load-data-set">load data set</h2><p>again，笔记中给出的下载代码看上去没法在win中运行，Suchan这里和Perceptron一节中一样，直接下载现成的<a href="https://github.com/microsoft/AI-For-Beginners/raw/c639951043c67fe7862f5c236d1e4f0cdf68202c/data/mnist.pkl.gz">zip文件</a><br>然后稍微替换一下dataset的加载方式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># with open(&#x27;./mnist.pkl&#x27;, &#x27;rb&#x27;) as mnist_pickle:</span></span><br><span class="line"><span class="comment">#     MNIST = pickle.load(mnist_pickle)</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">with</span> gzip.<span class="built_in">open</span>(<span class="string">&#x27;./mnist.pkl.gz&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> mnist_pickle:</span><br><span class="line">    MNIST = pickle.load(mnist_pickle)</span><br></pre></td></tr></table></figure><p>copy一下教程对数据的处理,why not?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(MNIST[<span class="string">&#x27;Train&#x27;</span>][<span class="string">&#x27;Features&#x27;</span>][<span class="number">0</span>][<span class="number">130</span>:<span class="number">180</span>])</span><br><span class="line"><span class="built_in">print</span>(MNIST[<span class="string">&#x27;Train&#x27;</span>][<span class="string">&#x27;Labels&#x27;</span>][<span class="number">0</span>])</span><br><span class="line">features = MNIST[<span class="string">&#x27;Train&#x27;</span>][<span class="string">&#x27;Features&#x27;</span>].astype(np.float32) / <span class="number">256.0</span></span><br><span class="line">labels = MNIST[<span class="string">&#x27;Train&#x27;</span>][<span class="string">&#x27;Labels&#x27;</span>]</span><br><span class="line">fig = plt.figure(figsize=(<span class="number">10</span>,<span class="number">5</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    ax = fig.add_subplot(<span class="number">1</span>,<span class="number">10</span>,i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(features[i].reshape(<span class="number">28</span>,<span class="number">28</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="process">process</h2><p>笔记中给出了一个已有的函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">set_mnist_pos_neg</span>(<span class="params">positive_label, negative_label</span>):</span><br><span class="line">    positive_indices = [i <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">enumerate</span>(MNIST[<span class="string">&#x27;Train&#x27;</span>][<span class="string">&#x27;Labels&#x27;</span>]) </span><br><span class="line">                          <span class="keyword">if</span> j == positive_label]</span><br><span class="line">    negative_indices = [i <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">enumerate</span>(MNIST[<span class="string">&#x27;Train&#x27;</span>][<span class="string">&#x27;Labels&#x27;</span>]) </span><br><span class="line">                          <span class="keyword">if</span> j == negative_label]</span><br><span class="line"></span><br><span class="line">    positive_images = MNIST[<span class="string">&#x27;Train&#x27;</span>][<span class="string">&#x27;Features&#x27;</span>][positive_indices]</span><br><span class="line">    negative_images = MNIST[<span class="string">&#x27;Train&#x27;</span>][<span class="string">&#x27;Features&#x27;</span>][negative_indices]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> positive_images, negative_images</span><br></pre></td></tr></table></figure><p>函数要求我们输入两个数字，一个<strong>这个数字</strong> 一个<strong>其他数字</strong><br>但是根据前文的思路来说，我们没办法将<strong>其他数字</strong>这个概念填入到这个函数中，我们不妨改造一个函数，用于输入一个数字，获取到它的数据集即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_mnist_pos</span>(<span class="params">lable</span>):</span><br><span class="line">    positive_indices = [i <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">enumerate</span>(MNIST[<span class="string">&#x27;Train&#x27;</span>][<span class="string">&#x27;Labels&#x27;</span>]) </span><br><span class="line">                          <span class="keyword">if</span> j == lable]</span><br><span class="line">    positive_images = MNIST[<span class="string">&#x27;Train&#x27;</span>][<span class="string">&#x27;Features&#x27;</span>][positive_indices]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> positive_images</span><br></pre></td></tr></table></figure><p>最终计算准确率之前，我们还要定义一个改造数据的函数：即我们最终的测试集也需要包含<strong>这个数字</strong>和<strong>其他数字</strong><br>回想我们这个架构的工作流程：对于<strong>这个数字</strong>，我们将输出一个正值，对于<strong>其他数字</strong>我们应该一个负值，那么很容易想到我们构造的label就应该是：对于<strong>当前数字</strong>我们给一个+1，对于<strong>其他数字</strong>我们给一个-1，到时候其做点乘，如果结果正确，二者点乘的结果就应该是正值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gen_test_data</span>(<span class="params">test_pos , test_label , this_num</span>):<span class="comment">#约定传入的test_poss:当前数字的数据集 ， test_label:+1 ， this_num:当前数字</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        <span class="keyword">if</span> i == this_num:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        other_num_pos = get_mnist_pos(i)</span><br><span class="line">        split_pre , split_post = np.split(other_num_pos, split_num)</span><br><span class="line">        test_pos = np.concatenate((test_pos, split_post),axis=<span class="number">0</span>)</span><br><span class="line">        test_label = np.concatenate((test_label , np.full(split_post.shape[<span class="number">0</span>] , -<span class="number">1</span>)),axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> test_pos , test_label</span><br></pre></td></tr></table></figure><p>然后我们来对每一个数字进行一次训练，将训练出来的weight返回出去</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_this_num</span>(<span class="params">this_num</span>):</span><br><span class="line">    this_num_pos = get_mnist_pos(this_num)<span class="comment">#获取到当前数字的数据集</span></span><br><span class="line">    other_num_pos = np.array([<span class="number">1</span>,<span class="number">1</span>])<span class="comment">#此处算是Suchan对python语法不太熟悉，有熟悉的读者朋友可以自行修改。大致思路就是：先给other_num_pos赋一个随便什么值，在之后获取其他数字的数据集时候不断填充进其他数据</span></span><br><span class="line">    flag = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        <span class="keyword">if</span> i == this_num:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line">        tem_pos = get_mnist_pos(i)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> flag:</span><br><span class="line">            flag = <span class="literal">False</span></span><br><span class="line">            other_num_pos = tem_pos</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            other_num_pos = np.concatenate((other_num_pos, tem_pos),axis=<span class="number">0</span>)</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">    train_x , test_x = np.split(this_num_pos, [<span class="number">2000</span>])<span class="comment">#此处前一节的教程的分割方式是 train_x, test_x = np.split(X, [ n*8//10])，导致在train的时候训练集太少，测试集太多，没有拟合好，提醒各位朋友在分割数据集的时候尽量手动check一下训练集和测试集的比例</span></span><br><span class="line">    <span class="comment">#或者使用sklearn.model_selection下的train_test_split</span></span><br><span class="line">    <span class="comment">#例如：features_train, features_test, labels_train, labels_test = train_test_split(data,labels,test_size=0.2)</span></span><br><span class="line">    train_x = np.c_[train_x,np.ones(<span class="built_in">len</span>(train_x))]<span class="comment"># 给输入向量添加一个高维度的1作为bias</span></span><br><span class="line">    tain_y = np.full((train_x.shape[<span class="number">0</span>]), this_num)</span><br><span class="line">    test_y = np.full((test_x.shape[<span class="number">0</span>]) , this_num)</span><br><span class="line">    other_num_pos = np.c_[other_num_pos,np.ones(<span class="built_in">len</span>(other_num_pos))]</span><br><span class="line">    weight = train(train_x , other_num_pos , <span class="number">1000</span>)</span><br><span class="line">    test_x , test_y = gen_test_data(test_x , test_y , this_num)<span class="comment">#给当前测试集添加其他数字做正反判断</span></span><br><span class="line">    ac = accuracy(weight, test_x, test_y)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125; : accuracy = &#123;&#125;&quot;</span>.<span class="built_in">format</span>(this_num, ac))</span><br><span class="line">    <span class="keyword">return</span> weight</span><br></pre></td></tr></table></figure><p>好了，经过以上代码，我们运行能够得到我们的结果：<br><img src="/img/AI-for-beginners/lab1/accuracy.png" alt="result" title="result"><br>看上去结果还是可以接受。</p><ul><li>至于最终定义一个函数来判断这里省略吧，其实也就是把全部的weight组合在一起，点乘输出一个向量，得到正值的就是我们的结果</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> AI_for_beginner_microsoft_lab </tag>
            
            <tag> Machine-learning_lab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI-for-beginners_1</title>
      <link href="/2024/08/09/AI-for-beginners-1/"/>
      <url>/2024/08/09/AI-for-beginners-1/</url>
      
        <content type="html"><![CDATA[<blockquote><p>lesson/3-NeuralNetworks/03-Perceptron</p></blockquote><h1>分类问题（classification）</h1><p>感知器（Perceptron）能够解决分类问题。从最简单的二元分类开始</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 引入必要的库</span></span><br><span class="line"><span class="keyword">import</span> pylab</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> gridspec</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> ipywidgets <span class="keyword">import</span> interact, interactive, fixed</span><br><span class="line"><span class="keyword">import</span> ipywidgets <span class="keyword">as</span> widgets</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"></span><br><span class="line"><span class="comment"># pick the seed for reproducability - change it to explore the effects of random variations</span></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure><h2 id="构造数据">构造数据</h2><p>教程中为了方便理解，给数据赋予了实际含义：用两个指标来判断肿瘤是良性还是恶性：时间和尺寸<br>即我们的任务就是训练一个感知器，它能够在给定一个肿瘤的 [时间、尺寸] 下，判断出肿瘤是良性还是恶性。下面的程序中用+1和-1来区分良性和恶性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">n = <span class="number">50</span></span><br><span class="line">X, Y = make_classification(n_samples = n, n_features=<span class="number">2</span>,</span><br><span class="line">                           n_redundant=<span class="number">0</span>, n_informative=<span class="number">2</span>, flip_y=<span class="number">0</span>)</span><br><span class="line">Y = Y*<span class="number">2</span>-<span class="number">1</span> <span class="comment"># convert initial 0/1 values into -1/1</span></span><br><span class="line">X = X.astype(np.float32); Y = Y.astype(np.int32) <span class="comment"># features - float, label - int</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Split the dataset into training and test</span></span><br><span class="line">train_x, test_x = np.split(X, [ n*<span class="number">8</span>//<span class="number">10</span>])</span><br><span class="line">train_labels, test_labels = np.split(Y, [n*<span class="number">8</span>//<span class="number">10</span>])</span><br><span class="line"><span class="built_in">print</span>(train_x.shape, train_labels.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Features:\n&quot;</span>,train_x[<span class="number">0</span>:<span class="number">4</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Labels:\n&quot;</span>,train_labels[<span class="number">0</span>:<span class="number">4</span>])</span><br></pre></td></tr></table></figure><p><img src="/img/AI-for-beginners/bclassificationDataPlot.png" alt="bclassificationDataPlot" title="bclassificationDataPlot"><br>可以看到我们产生了一些数据，并且用不同的颜色划分了他们的类别（classification）<br>（忽略了画图的代码，这类工具类的代码用到的时候copy一下就好）</p><h2 id="感知器Perceptron">感知器Perceptron</h2><p>将问题抽象：即对于一个输入向量x（在之前肿瘤的情境下，这里的输入就是一个二维的向量[age , size]），我们需要找到一个矩阵W和一个函数f使得对于任意的输入x，输出我们需要的分类<br>$$y(x) = f(W^Tx)$$</p><p>对于二元分类，很容易想到f可以使用一个分段函数来分类</p> $$ f(x) =  \begin{cases} +1,\,\,x>0\\ -1,\,\,x\le0\\ \end{cases} $$ <p>而对于矩阵W的理解，可以认为是对向量x的一个变换，将两个类别的分界点通过W变换到f需要的分界点（即x == 0）处</p><blockquote><p>此处Suchan想到的一个疑问是：$W^T x$得到的仍然是一个向量，如何放到f中计算：</p></blockquote><ul><li><blockquote><p>对于二元分类来说是一维的，即f的输入仅是一个数，所以W就应该是一个向量，[1 * n]与[n * 1]的向量乘积将得到一个数。倘若是多元的分类（例如二维，四元分类），则对于f则需要对应维度的x来表示这个点应当归属到哪个class中</p></blockquote></li></ul><hr><p>事实上，<strong>一个感知器是线性的</strong>，在获得W之后我们通常还需要一个常量（bias），即完整的式子是<br>$$y(x) = f(W^T x + b)$$<br>熟悉线性代数的朋友应该能反应过来，可以通过增加维度的方式，将bias糅合到W中，以下是一个例子</p>$$\begin{bmatrix} 1 & 2 \\3&4\end{bmatrix} \begin{bmatrix} 1 \\ 1\end{bmatrix} + \begin{bmatrix} 1 \\ 1\end{bmatrix} = \begin{bmatrix} 4 \\ 8\end{bmatrix}$$<p>可以等价于</p>$$\begin{bmatrix} 1 & 2 & 1\\3&4& 1\end{bmatrix} \begin{bmatrix} 1 \\ 1 \\ 1\end{bmatrix} = \begin{bmatrix} 4 \\ 8\end{bmatrix}$$<p>所以我们直接给数据增加一个维度，这个维度的值始终为1，就将bias这一项变成了简单的矩阵乘法</p><h2 id="训练模型">训练模型</h2><p>训练模型的过程就是找到一个矩阵W，使得通过这个W获得的结果在训练集(train_data)上错误(error)最小</p><h3 id="定义损失函数-loss">定义损失函数(loss)</h3><p>通常对于回归问题来说，损失函数可以是直接对输出(output)与标签(label)的差的绝对值求和，或者对其平方求和</p><blockquote><p>回归问题(regression)：对于给定的输入，输出一个值。训练的模型通常用于预测值，如股票等</p></blockquote><hr><p>通常对于分类问题，损失函数通常使用交叉熵（之后介绍）<br>对于此处的问题，我们借助于输出的分类仅有可能是+1或者-1来做文章：$t \in \{ +1 , -1 \} $<br>规定当模型的输出与标签不相符时</p>$$E(w) = - \sum W^t x t$$<p>简单来说就是当输出与标签不相符时，$xt$的结果小于0，与前面的负号相抵消得到一个正值，loss越大，效果越差。<br>只是不好的点在于：这样定义的loss只是告诉了你当前的结果是差的，但是没有反应到底有多差。比如我们通常希望在$W_1$的结果下loss是1，当更新到$W_2$的时候loss是0.5，这样会在一定程度上告诉我们训练模型是否有进展</p><h3 id="修改权重矩阵W">修改权重矩阵W</h3><p>最常用的就是梯度下降法(SGD)，简单来说就是将loss函数对要更新的参数求偏导后，往梯度下降的方向更新参数，从而获得更小的loss<br>对于这个case下的loss函数，我们对W求偏导后得到<br>$$\frac{\partial E}{\partial W} =  -\sum xt $$<br>所以我们更新W的方式就是<br>$$W^{T+1} = W^T - \eta \frac{\partial E}{\partial W} = W^T + \eta\sum xt$$<br>式中$ \eta $代表学习速率，或者更新步长，通常是一个常数(在特殊的优化方法下将是可变的)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">positive_examples, negative_examples, num_iterations = <span class="number">100</span></span>):</span><br><span class="line">    num_dims = positive_examples.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize weights. </span></span><br><span class="line">    <span class="comment"># We initialize with 0 for simplicity, but random initialization is also a good idea</span></span><br><span class="line">    weights = np.zeros((num_dims,<span class="number">1</span>)) </span><br><span class="line">    </span><br><span class="line">    pos_count = positive_examples.shape[<span class="number">0</span>]</span><br><span class="line">    neg_count = negative_examples.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    report_frequency = <span class="number">10</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iterations):</span><br><span class="line">        <span class="comment"># Pick one positive and one negative example</span></span><br><span class="line">        pos = random.choice(positive_examples)</span><br><span class="line">        neg = random.choice(negative_examples)</span><br><span class="line"></span><br><span class="line">        z = np.dot(pos, weights)   </span><br><span class="line">        <span class="keyword">if</span> z &lt; <span class="number">0</span>: <span class="comment"># 正值标签获得了负值输出</span></span><br><span class="line">            weights = weights + pos.reshape(weights.shape)</span><br><span class="line"></span><br><span class="line">        z  = np.dot(neg, weights)</span><br><span class="line">        <span class="keyword">if</span> z &gt;= <span class="number">0</span>: <span class="comment"># 负值标签获得了正值输出</span></span><br><span class="line">            weights = weights - neg.reshape(weights.shape)</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># Periodically, print out the current accuracy on all examples </span></span><br><span class="line">        <span class="keyword">if</span> i % report_frequency == <span class="number">0</span>:             </span><br><span class="line">            pos_out = np.dot(positive_examples, weights)</span><br><span class="line">            neg_out = np.dot(negative_examples, weights)        </span><br><span class="line">            pos_correct = (pos_out &gt;= <span class="number">0</span>).<span class="built_in">sum</span>() / <span class="built_in">float</span>(pos_count)</span><br><span class="line">            neg_correct = (neg_out &lt; <span class="number">0</span>).<span class="built_in">sum</span>() / <span class="built_in">float</span>(neg_count)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Iteration=&#123;&#125;, pos correct=&#123;&#125;, neg correct=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i,pos_correct,neg_correct))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure><p>运行教程中的程序可以看到经历几轮的训练，正确率在逐步上升<br><img src="/img/AI-for-beginners/exampleResult.png" alt="exampleResult" title="exampleResult"><br>通过其可视化数据来看，我们训练的模型确实找到了一个分界点(线)来将数据分类<br><img src="/img/AI-for-beginners/visualizeResult.png" alt="visualizeResult" title="visualizeResult"></p><blockquote><p>相信通过上图，读者朋友能够很直观的看出来，为什么前文提到<strong>感知器是线性的</strong></p></blockquote><h2 id="检验结果">检验结果</h2><p>教程中为我们编写好了accuracy函数帮助我们后续检测我们自己训练的模型的准确率有多高</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">weights, test_x, test_labels</span>):</span><br><span class="line">    res = np.dot(np.c_[test_x,np.ones(<span class="built_in">len</span>(test_x))],weights)</span><br><span class="line">    <span class="keyword">return</span> (res.reshape(test_labels.shape)*test_labels&gt;=<span class="number">0</span>).<span class="built_in">sum</span>()/<span class="built_in">float</span>(<span class="built_in">len</span>(test_labels))</span><br><span class="line"></span><br><span class="line">accuracy(wts, test_x, test_labels)</span><br></pre></td></tr></table></figure><h2 id="limitation-of-the-Perceptron">limitation of the Perceptron</h2><p>通过上面的可视化结果我们可以联想到：当前case由于其特性能够找到一条直线将数据分为两个类，倘若分界线是一条曲线该如何？<br>确实，一个经典的案例就是异或问题(XOR)<br><img src="/img/AI-for-beginners/visualXOR.png" alt="visualXOR" title="visualXOR"><br>当数据真值呈异或分布的时候，一个线性的感知器无论如何划线，其正确率始终无法高于75%。其解决办法将会在之后的小节讲述：多层感知器（深度学习）</p><h1>较复杂的例子：MNIST</h1><p>MNIST是一个手写数字的数据库。该数据库常用于做数字识别。<br>所有的数字都用28 * 28像素的灰度图表示<br>我们要做的就是利用MNIST训练一个模型，使其能够识别其中的数字<br><a href="https://github.com/microsoft/AI-For-Beginners/raw/c639951043c67fe7862f5c236d1e4f0cdf68202c/data/mnist.pkl.gz">数据库下载连接</a>(原教程中被不小心删除一直没有添加)</p><h2 id="加载数据库">加载数据库</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> gzip.<span class="built_in">open</span>(<span class="string">&#x27;./lab/mnist.pkl.gz&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> mnist_pickle:<span class="comment">#下载了数据库后存放的位置</span></span><br><span class="line">    MNIST = pickle.load(mnist_pickle)</span><br></pre></td></tr></table></figure><p>通过教程的绘图代码可以形象看到数据的形式<br><img src="/img/AI-for-beginners/dataplot.png" alt="dataplot" title="dataplot"></p><h2 id="训练">训练</h2><p>本例中的分类模型是二元分类，教程尝试的是拿出两个数字的灰度图进行分类，一个数字做+1一个数字做-1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">set_mnist_pos_neg</span>(<span class="params">positive_label, negative_label</span>):</span><br><span class="line">    positive_indices = [i <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">enumerate</span>(MNIST[<span class="string">&#x27;Train&#x27;</span>][<span class="string">&#x27;Labels&#x27;</span>]) </span><br><span class="line">                          <span class="keyword">if</span> j == positive_label]</span><br><span class="line">    negative_indices = [i <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">enumerate</span>(MNIST[<span class="string">&#x27;Train&#x27;</span>][<span class="string">&#x27;Labels&#x27;</span>]) </span><br><span class="line">                          <span class="keyword">if</span> j == negative_label]</span><br><span class="line"></span><br><span class="line">    positive_images = MNIST[<span class="string">&#x27;Train&#x27;</span>][<span class="string">&#x27;Features&#x27;</span>][positive_indices]</span><br><span class="line">    negative_images = MNIST[<span class="string">&#x27;Train&#x27;</span>][<span class="string">&#x27;Features&#x27;</span>][negative_indices]</span><br><span class="line"></span><br><span class="line">    fig = pylab.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    pylab.imshow(positive_images[<span class="number">0</span>].reshape(<span class="number">28</span>,<span class="number">28</span>), cmap=<span class="string">&#x27;gray&#x27;</span>, interpolation=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line">    ax.set_xticks([])</span><br><span class="line">    ax.set_yticks([])</span><br><span class="line">    ax = fig.add_subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    pylab.imshow(negative_images[<span class="number">0</span>].reshape(<span class="number">28</span>,<span class="number">28</span>), cmap=<span class="string">&#x27;gray&#x27;</span>, interpolation=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line">    ax.set_xticks([])</span><br><span class="line">    ax.set_yticks([])</span><br><span class="line">    pylab.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> positive_images, negative_images</span><br><span class="line"></span><br><span class="line">pos1,neg1 = set_mnist_pos_neg(<span class="number">1</span>,<span class="number">0</span>)<span class="comment">#用0和1来做分类</span></span><br><span class="line">train(pos1 , neg1)<span class="comment">#训练模型</span></span><br></pre></td></tr></table></figure><p>运行程序可以看到训练的准确率在上升，此时模型可以辨别1和0两个数字</p><h2 id="讨论">讨论</h2><p>由于一些原因，模型对数字2和5更难区分。为了理解其中的原因，尝试使用PCA分析来降低feature的维度，使得将高维的输入(MNIST每个图片是一个784维的向量)降低到两个维度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pca_analysis</span>(<span class="params">positive_label, negative_label</span>):</span><br><span class="line">    positive_images, negative_images = set_mnist_pos_neg(positive_label, negative_label)</span><br><span class="line">    M = np.append(positive_images, negative_images, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    mypca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">    mypca.fit(M)</span><br><span class="line">    </span><br><span class="line">    pos_points = mypca.transform(positive_images[:<span class="number">200</span>])</span><br><span class="line">    neg_points = mypca.transform(negative_images[:<span class="number">200</span>])</span><br><span class="line"></span><br><span class="line">    pylab.plot(pos_points[:,<span class="number">0</span>], pos_points[:,<span class="number">1</span>], <span class="string">&#x27;bo&#x27;</span>)</span><br><span class="line">    pylab.plot(neg_points[:,<span class="number">0</span>], neg_points[:,<span class="number">1</span>], <span class="string">&#x27;ro&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pca_analysis(<span class="number">1</span>,<span class="number">0</span>)</span><br><span class="line">pca_analysis(<span class="number">2</span>,<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p><img src="/img/AI-for-beginners/featureBetween0_1.png" alt="featureBetween0_1" title="featureBetween0_1"><br><img src="/img/AI-for-beginners/featureBetween2_5.png" alt="featureBetween2_5" title="featureBetween2_5"></p><p>可以看到，数字2和5的点相对很难找到一个线性的分界线</p><p>至此，我们学习了：</p><ul><li>最简单的神经网路结构：单层感知器</li><li>通过梯度下降的方式，手动完成了一个感知器</li><li>简单的线性感知器，解决了手写数字识别问题</li></ul><h1>Creadits</h1><p>文章所有内容均是Suchan学习<a href="https://github.com/microsoft/AI-For-Beginners">AI-for-begginners</a>所得</p>]]></content>
      
      
      
        <tags>
            
            <tag> AI_for_beginner_microsoft_note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI-for-beginners_0</title>
      <link href="/2024/08/09/AI-forbeginners-0/"/>
      <url>/2024/08/09/AI-forbeginners-0/</url>
      
        <content type="html"><![CDATA[<h1>Background</h1><p>之前学习李宏毅机器学习的视频课，感觉无穷无尽的视频很容易分散注意力，加之配课程要求的环境有一些阻力，演化成了只看不练的假把式，深思熟虑后认为李宏毅老师的课程不太适合我目前的学习状态。课程讲得固然清晰，但是奈何没法长时间集中注意学习。痛定思痛，找到了微软的AI-for-beginners课程，决定跟着每一讲快速过一遍知识点，把每一个lab做下去。<br>note：截止这一篇文章撰写的时候Suchan做了两讲的内容，发现：</p><ul><li>李宏毅老师的课程将挺多知识点都讲解的十分透彻，自己在看课程的时候遇到的知识点都很快理解并往下做</li><li>微软共有三个AI的课程，分别是<a href="https://github.com/microsoft/ML-For-Beginners">MachineLearning-for-beginners</a>、<a href="https://github.com/microsoft/AI-For-Beginners">AI-for-beginners</a>以及<a href="https://github.com/microsoft/generative-ai-for-beginners">generative-ai-for-beginners</a>，遗憾的是，Suchan选中的AI-for-beginners没有中文教程，只能硬着头皮啃英文版</li><li>再次遗憾的是，Suchan找了挺多网站也没有找到AI-for-beginners里面Lab的solution，只能自己摸索（有找到的小伙伴麻烦评论分享一下呢）</li></ul><h2 id="ok废话少说，启动！">ok废话少说，启动！</h2><h1>环境搭建</h1><p>教程中提供了三种运行代码的方式，本地构建、云上运行以及云上GPU运行。盲猜后两种云不会是免费的午餐，果断选择了本地方式。</p><h2 id="安装conda">安装conda</h2><p>登录anaconda的<a href="https://www.anaconda.com/download">官网</a>下载，anaconda是一个集成的工具软件。<br>直接跳过登录下载即可<br><img src="/img/AI-for-beginners/conda.png" alt="conda" title="conda"><br>找到自己的平台一路下载安装<br><img src="/img/AI-for-beginners/condaDownload.png" alt="condaDownload" title="condaDownload"><br>直到任务栏中可用<br><img src="/img/AI-for-beginners/condaDownloadSuccess.png" alt="condaDownloadSuccess" title="condaDownloadSuccess"><br>随后跟着课程中的命令，最好把别人的仓库fork一份再克隆自己的，这样自己写的lab提交到自己的仓库可以随时方便查看，会更加清晰一些</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> http://github.com/microsoft/ai-for-beginners</span><br><span class="line"><span class="built_in">cd</span> ai-for-beginners</span><br><span class="line">conda <span class="built_in">env</span> create --name ai4beg --file .devcontainer/environment.yml</span><br><span class="line">conda activate ai4beg</span><br></pre></td></tr></table></figure><h2 id="编辑器">编辑器</h2><p>教程中推荐使用vscode + python Extension来运行，作者发现直接使用pycharm会方便一些（jet brain yyds！！），省去了不少配置的麻烦，一键下载jupyter notebook，唯一的缺点就是需要一些银子。当然也可以遵从教程的做法，毕竟vscode是免费使用的</p><blockquote><p>此处就不展示如何下载pycharm以及配置vscode了，相信你可以的</p></blockquote><h3 id="检查环境是否成功">检查环境是否成功</h3><p>让我们不妨直接跳到教程的第三节:3-NeuralNetworks/03-Perceptron/Perceptron.ipynb(前两节大多在讲解AI的发展史以及一两个无关紧要的算法引导。感觉不看也无伤大雅)随便找到一段代码，例如下图引入一些库的代码片段：<br><img src="/img/AI-for-beginners/environmentCheck.png" alt="environmentCheck" title="environmentCheck"><br>点击运行后看到执行成功就是环境配好了，如果报错一些库引入失败，可以尝试在终端中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install xxx</span><br></pre></td></tr></table></figure><p>通常xxx就是你引入的库的名称，如果没法install，尝试把报错复制搜索，能找到具体的install的名称</p><h1>What’s More</h1><p>至此环境搭好，可以开始AI之旅<br>每一节都会有6道题的<a href="https://red-field-0a6ddfd03.1.azurestaticapps.net/">quiz</a>，pre &amp; post Lecture，大多是一些基本概念</p>]]></content>
      
      
      
        <tags>
            
            <tag> AI_for_beginner_microsoft_note </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Machine_learning_1</title>
      <link href="/2024/08/04/Machine-learning-1/"/>
      <url>/2024/08/04/Machine-learning-1/</url>
      
        <content type="html"><![CDATA[<h1>李宏毅机器学习笔记（一）</h1><span id="more"></span><h1>问题种类：regression 、 classification 、 structuredLearning</h1><p>$$<br>y = w * x + b (linear)<br>$$</p><ul><li>训练合适的w和b</li></ul><h2 id="定义loss函数：">定义loss函数：</h2><ul><li>MAE： regression中预测值与label之间差的绝对值和</li><li>MSE： regression中预测值与label之间差的平方和</li><li>Cross-Entropy：classification中预测值与label的交叉熵</li></ul><h2 id="training：找到loss最小的feature">training：找到loss最小的feature</h2><h2 id="optimize">optimize</h2><ul><li>通常使用梯度下降法：对各个feature求偏导得到梯度，向梯度减小的方向修改参数</li><li>tips：对于斜率变化小的case，在梯度下降的过程中使用相同的步长将很难达到最优的loss。通常使用adam(pytorch自带)、冲量梯度下降法来优化</li></ul><h1>DNN</h1><p>上述linear的方程没法预测更多一般的情况，通常采用多个ReLU函数或者Sigmoid函数来逼近真实的分布</p><h3 id="sigmoid">sigmoid</h3><p>$$y = b + \sum_i c_i sigmoid(y_j)$$<br>上式$y_j$也可以写成$y_j = b + \sum_j w_jx_j $<br>有$$y = b + \sum_i c_i sigmoid(b_i + \sum_j w_{ij}x_i)$$<br>写成矩阵形式<br>$$<br>y = b + \begin{bmatrix} c\end{bmatrix}^T + sigmoid(b + \begin{bmatrix} W\end{bmatrix}\begin{bmatrix} x\end{bmatrix})<br>$$</p><ul><li>上式的$y$当作下一层的输入，又可以套一层神经网路，即是简单的DNN模型</li></ul><h3 id="批处理">批处理</h3><p>训练时候将数据分成多个batch，每个batch计算出一个loss，根据loss来计算梯度，更新feature。</p><ul><li>全部batch计算完称为一个epoch</li></ul><h1>CNN</h1><p>CNN通常用于图像操作，输入是一幅图像</p><h2 id="卷积">卷积</h2><p>对于输入图像看作一个矩阵$\begin{bmatrix} Image\end{bmatrix}$ , 定义一个卷积核矩阵$\begin{bmatrix} Filter\end{bmatrix}$与其上的数字相乘相加得到一个值称为一次卷积<br>通过训练出一系列的卷积核一层一层的对图像进行卷积运算提取相关的特征</p><h3 id="一些问题及解决">一些问题及解决</h3><ul><li>卷积没法超出边界导致卷积结果大小 &lt; 原图大小 =&gt; 通过给图像边界补0（padding）</li><li>每次卷积都对整个图像操作，计算量大，参数多，容易overfitting =&gt; 设定Receptive Field</li></ul><h4 id="Receptive-Field相关">Receptive Field相关</h4><p>针对不同的特征，课程中提到的“识别鸟”的任务，可以训练一些特征如鸟喙、鸟腿、翅膀。<br>将输入图像划分成多个Receptive Field，每个Receptive Field由识别多种特征的神经元来检测。<br>相同特征的神经元共享参数，检测不同的Receptive Field（某个特征可能出现在输入图像的不同位置，所以每个Receptive Field都要放检测某一特征的神经元）</p><h4 id="池化">池化</h4><p>有时候运算量过大时，可以使用池化减少运算量</p><ul><li>池化挑选一次卷积的结果中的某一个值：最大值（max pooling） 最小值（min pooling） 平均值（mean pooling）</li><li>但是减少运算量的同时会丢失精度</li></ul><h1>self attention</h1><p>$$<br>k = W_k * I<br>$$</p><p>$$<br>q = W_q * I<br>$$</p><p>$$<br>v = W_v * I<br>$$<br>计算第i个输入与第j个输入的相关性<br>$$<br>\alpha_{ij} = dot(q_i , k_j )<br>$$<br>得到相关性后，该层的输出<br>$$b_i = \sum_i \alpha_{ij} v_i$$<br>将得到的结果通过softmax操作得到一个结果的置信值</p><h2 id="multi-head-self-attention">multi-head self-attention</h2><p>self-attention计算 q、k、v, multi-head将计算$q_1,q_2,k_1,k_2,v_1,v_2$<br>计算相关性：<br>$$\alpha_{ijk} = dot(q_{ik} , k_{jk})$$<br>计算输出：<br>$$b_{ik} = \sum_i \alpha_{ijk} v_k$$<br>将得到的$b_i1 , b_i2$拼接：<br>$$b_i = \begin{bmatrix} W_b\end{bmatrix} \begin{bmatrix} b_{i1}\b_{i2}\end{bmatrix}$$</p><h2 id="positional-encoding">positional encoding</h2><p>给输入向量$I$加入位置信息<br>$$I + e^i$$</p><h1>Transformer : encoder + decoder</h1><h2 id="encoder">encoder</h2><p>$\begin{bmatrix} x_1 \ x_2 \…\x_n\end{bmatrix}$=&gt; block =&gt; $\begin{bmatrix} y_1 \ y_2 \…\y_n\end{bmatrix}$</p><h3 id="block">block</h3><p>$\begin{bmatrix} x_1 \ x_2 \…\x_n\end{bmatrix}$ =&gt; self-attention =&gt; $\begin{bmatrix} x_1’ \ x_2’ \…\x_n’\end{bmatrix}$</p><p>$\begin{bmatrix} x_1’ \ x_2’ \…\x_n’\end{bmatrix}$ + $\begin{bmatrix} x_1 \ x_2 \…\x_n\end{bmatrix}$ =&gt; norm</p><ul><li>norm :${ x - \mu \over \sigma}$</li></ul><h2 id="decoder">decoder</h2><p>获取encoder的输出作为输入，产生transformer真正的输出<br>$\begin{bmatrix} x_1’ \ x_2’ \…\x_n’\end{bmatrix}$ =&gt; decoder<br>token =&gt; decoder<br>特殊的输入：给decoder的token中有两个特殊的字符：BOS（begin of sequence）和END<br>masked-self-attention：通常的self-attention每个输出都会看过所有的输入，masked-self-attention只会看之前的输入，不看之后的输入<br>vocabulary-list：transformer输出的token组成的词典</p><h3 id="decoder过程：">decoder过程：</h3><ul><li>开始时获取encoder的输出结果，token使用BOS，经过masked-self-attention</li><li>masked-self-attention的输出经过soft-max得到概率分布，选取最大的值对应的vocabulary-list的token作为decoder的输出</li><li>之后的轮次，将decoder每次的输出，累计作为新的输入再次输出，直到输出END为止</li></ul><h3 id="改进">改进</h3><p>使用这种方式是顺序串行产生结果，效率较低<br>考虑并行化，NAT（non-autoregressive）：难点在于不确定输出的长度</p><ul><li>1、另外训练一个神经网络，用于产生输出的长度N</li><li>2、给定一个最大值的输出数量，截取到END作为输出</li></ul><h2 id="encoder和decoder的信息传递">encoder和decoder的信息传递</h2><p>encoder输出$\begin{bmatrix} x_1’ \ x_2’ \…\x_n’\end{bmatrix}$<br>参考self-attention的思想：<br>$$k = W_k \begin{bmatrix} x_1’ \ x_2’ \…\x_n’\end{bmatrix}$$<br>$$v = W_v \begin{bmatrix} x_1’ \ x_2’ \…\x_n’\end{bmatrix}$$<br>$$q = W_q \begin{bmatrix} token’\end{bmatrix}(token’是token经过masked-self-attention得到的向量)$$<br>最终输出$$b = \sum_i dot(k_i , q) * v_i$$</p><h2 id="一些问题">一些问题</h2><ul><li>1、串行输出的decoder在输出一个错误token后将会一步错，步步错<br>ans：在训练时不用decoder的输出作为输入，而是使用ground truth作为输入</li><li>2、有时transformer会忽略输入的一部分或者获得了乱序的输入（语音识别时必须有序才有意义）<br>ans：使用guided-attention，训练时强迫顺序看完训练资料</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> MachineLearning </tag>
            
            <tag> 李宏毅机器学习笔记 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title>about Suchan</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[<h1>About Suchan’s Home</h1><p>机缘巧合之下，有幸读到《暗时间》这本好书，大受启发，书中提到了许多Suchan平时自己也想到了的观点，但是远远没有作者刘未鹏先生那般凝练、系统，在这本书的强烈建议下，恰逢Suchan也有一些计算机相关的知识和技能，遂于2024年8月3日开始搭建本站。<br>奈何Suchan确实不太擅长前端网页技术，本着内容第一、美观第二的原则，跟着网上的一些教程用hexo连夜起草了这个网站，只能说美化网站的任务，是一个永恒的TODO<br>写出一些精华的文章，结识一些行业内的大牛，想必是每一个博客最初的梦想，Suchan自知在达到“稳定更新”之前什么梦想都是遥不可及的，所以本站的初心只是激励Suchan勤思考、多记录，倘若有幸有一些文章（如果真的更新出来了的话）能被您看到，那将是Suchan不可多得的荣幸。希望来到本站的你，能够与Suchan一同交流进步，共勉~<br>本站目前构想的是编写以下几个板块：</p><ul><li>机器学习相关的学习经验</li><li>游戏引擎的开发</li><li>游戏的开发（主要使用unity）</li><li>一些美术（Maybe）</li><li>一些读书笔记</li></ul><blockquote><p>技术栈啊，都学杂了~</p></blockquote><h1>About SuchanTso</h1><ul><li>昵称：SuchanTso</li><li>性别： ♂</li><li>出生日期：199x.6.9</li><li>邮箱：suchantso@zju.edu.cn</li><li>籍贯：云南人（是的家里有大象）</li><li>技术栈：主要还是cpp，最近学习机器学习，恶补python中</li><li>mbti：ENTJ。个人倾向于认为mbti是一个人对自己一段时间的总结，相比其他玄学要更加reality一些</li><li>SuchanTso是什么含义：SuchanTso是我短短二十几年中连续更换的第N个英文名了，其中不乏各种中二的代号，最后进化到了取自名字谐音的Suchan，至于Tso，则是姓氏“左”的英文（又高调又低调捏~</li></ul><h2 id="一些履历">一些履历</h2><p>Suchan手里的是小镇做题家剧本</p><ul><li>高考进入浙江大学信息工程专业</li><li>本科毕业两年于小x书当任程序员，主要负责自研游戏引擎的特效开发，所以有一些游戏引擎相关的技术栈。如果您用过该app的“文字”功能，那么很荣幸，Suchan参与开发了其中的大多数样式呢~</li><li>建站这年考取浙江大学攻读人工智能的研究生。遂加入了AI炼丹大军。</li></ul><p>TO BE CONTINUE…</p>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>archives</title>
      <link href="/archives/index.html"/>
      <url>/archives/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>link</title>
      <link href="/link/index.html"/>
      <url>/link/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>tags</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>
